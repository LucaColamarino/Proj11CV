{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d32c3b8",
   "metadata": {},
   "source": [
    "## Code based on the papers in the references of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62018e06",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from dataset import CityscapesFineDataset, LostAndFoundDataset, COLORS, PALETTE2ID, CITYSCAPES_19_TO_7_MACRO, rgb_to_id\n",
    "from torchmetrics.classification import MulticlassJaccardIndex\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc7a761",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebf04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\") if torch.cuda.is_available() else print(\"Using CPU\")\n",
    "\n",
    "# Training Parameters\n",
    "batch_size = 8 # Adjusted batch size for better performance\n",
    "model_output_classes = 8 # Number of classes\n",
    "epochs = 1\n",
    "resize = (256, 512)\n",
    "checkpoint_path = 'deeplabv3_cityscapes_fine.pth'\n",
    "best_miou = 0.0 # To keep track of the best model based on validation mIoU\n",
    "\n",
    "# Optimizer Parameters (from \"Road Obstacle Detection based on Unknown Objectness Scores\" paper inspiration)\n",
    "initial_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "poly_power = 0.9 # Power for the \"poly\" learning rate policy\n",
    "\n",
    "# Conformal Prediction Parameters (from \"A Gentle Introduction to Conformal Prediction...\" paper)\n",
    "alpha = 0.1 # Alpha for Conformal Prediction (1 - alpha confidence level, e.g., 0.1 for 90% coverage)\n",
    "\n",
    "# Unknown Obstacle Detection Parameters\n",
    "UNKNOWN_OBSTACLE_ID = 7 # Using num_classes as the ID for unknown obstacles\n",
    "ALL_COLORS_FOR_VISUALIZATION = COLORS.tolist() # This will pick up the COLORS from dataset.py\n",
    "anomaly_threshold_uos = 0.01 # Threshold for anomaly detection in unknown obstacle detection\n",
    "# Example image paths for inference (For Lost and Found dataset we need to change the rgb_to_id mapping)\n",
    "example_image_path = 'datasets/realcityscapes/leftImg8bit/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png'\n",
    "example_ground_truth_path = 'datasets/realcityscapes/gtFine/val/frankfurt/frankfurt_000000_000294_gtFine_labelTrainIds.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8cd992",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bbab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_lr_scheduler(optimizer, initial_lr, current_iter, total_iter, power=0.9):\n",
    "    \"\"\"\n",
    "    Implements the \"poly\" learning rate policy: \n",
    "    lr = initial_lr * (1 - iter/total_iters)^power, clipped to avoid negatives.\n",
    "    \"\"\"\n",
    "    # Evita che current_iter superi total_iter\n",
    "    ratio = min(current_iter / total_iter, 1.0)\n",
    "    \n",
    "    # Calcolo LR (sempre positivo e reale)\n",
    "    lr = initial_lr * (1 - ratio) ** power\n",
    "    lr = max(float(lr), 1e-8)  # opzionale: evita LR troppo vicino a 0\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34faba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_segmap(pred_mask, colors_list=ALL_COLORS_FOR_VISUALIZATION, ignore_id=255):\n",
    "    \"\"\"\n",
    "    Decodes a segmentation mask (ID-based) into an RGB image using predefined colors.\n",
    "    Args:\n",
    "        pred_mask (np.array): Predicted segmentation mask (H, W) with class IDs.\n",
    "        colors_list (list): List of RGB color tuples/lists for each class ID. Defaults to ALL_COLORS_FOR_VISUALIZATION.\n",
    "        ignore_id (int): The ID to be ignored (e.g., 255 for Cityscapes void pixels).\n",
    "    Returns:\n",
    "        np.array: RGB image (H, W, 3).\n",
    "    \"\"\"\n",
    "    h, w = pred_mask.shape\n",
    "    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for label_id in np.unique(pred_mask):\n",
    "        if label_id == ignore_id: # Handle ignore_index\n",
    "            color_mask[pred_mask == label_id] = [0, 0, 0] # Black for ignored pixels\n",
    "        elif 0 <= label_id < len(colors_list):\n",
    "            color_mask[pred_mask == label_id] = colors_list[label_id]\n",
    "        else:\n",
    "            # Fallback for unexpected labels (shouldn't happen if UNKNOWN_OBSTACLE_ID is handled correctly)\n",
    "            print(f\"Warning: Unexpected label ID {label_id} found in prediction mask. Mapping to blue.\")\n",
    "            color_mask[pred_mask == label_id] = [0, 0, 255] # Blue for unmapped unexpected\n",
    "    return color_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9af173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, num_classes_for_metric, ignore_index=255):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a given dataloader and computes the average loss and mIoU.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Initialize JaccardIndex (IoU) metric.\n",
    "    # num_classes_for_metric should be 7 for your 7 macro classes (0-6).\n",
    "    metric = MulticlassJaccardIndex(num_classes=num_classes_for_metric, ignore_index=ignore_index, average='macro', validate_args=False).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            metric.update(preds, masks)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    mean_iou = metric.compute()\n",
    "\n",
    "    model.train()\n",
    "    return avg_loss, mean_iou.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1aa25",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations for input images\n",
    "transform = transforms.Compose([\n",
    "    # ColorJitter is for images only, keep it here.\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(), # Convert PIL Image to Tensor\n",
    "    # Normalization for ImageNet pre-trained models (DeepLabV3 uses ResNet50 backbone)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Using CityscapesFineDataset from dataset.py\n",
    "train_ds = CityscapesFineDataset(root='datasets/realcityscapes', split='train', transform=transform, resize=resize, unknown_obstacle_id=UNKNOWN_OBSTACLE_ID)\n",
    "val_ds = CityscapesFineDataset(root='datasets/realcityscapes', split='val', transform=transform, resize=resize, unknown_obstacle_id=UNKNOWN_OBSTACLE_ID)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4, # This can be adjusted based on your system's capabilities\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True, # Set to True for better performance on systems with multiple workers\n",
    "    prefetch_factor=4, # Prefetch factor for better performance\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, # No need to shuffle validation data\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    ")\n",
    "print(\"Datasets loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40d734",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DeepLabV3 with ImageNet pre-trained weights for the ResNet50 backbone.\n",
    "# This helps in transfer learning and often leads to faster convergence and better performance.\n",
    "model = deeplabv3_resnet50(weights_backbone=ResNet50_Weights.IMAGENET1K_V1, num_classes=model_output_classes).to(device)\n",
    "model = model.to(memory_format=torch.channels_last) # Optimize memory layout for NVIDIA GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b6ae5b",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer: SGD with momentum and weight decay as specified in the paper\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255) # ignore_index=255 for unlabeled pixels\n",
    "scaler = torch.cuda.amp.GradScaler() # For mixed precision training\n",
    "\n",
    "# Checkpoint Handling\n",
    "start_epoch = 0\n",
    "current_iteration = 0 # Initialize current_iteration for learning rate scheduler\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    ck = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(ck['model'])\n",
    "    optimizer.load_state_dict(ck['optimizer'])\n",
    "    scaler.load_state_dict(ck['scaler'])\n",
    "    start_epoch = ck['epoch'] + 1\n",
    "    if 'best_miou' in ck:\n",
    "        best_miou = ck['best_miou']\n",
    "    if 'current_iteration' in ck:\n",
    "        current_iteration = ck['current_iteration']\n",
    "    print(f\"Restarting from epoch {start_epoch}, best mIoU until now: {best_miou:.4f}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting training from zero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "print(\"Starting training...\")\n",
    "total_iterations = epochs * len(train_loader) # Define total_iterations here for scheduler\n",
    "\n",
    "patience = 5 # Patience for early stopping\n",
    "epochs_no_improve = 0 # Counter for early stopping\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{start_epoch+epochs}\")\n",
    "    for images, masks in loop:\n",
    "        current_iteration += 1\n",
    "        current_lr = poly_lr_scheduler(optimizer, initial_lr, current_iteration, total_iterations, power=poly_power)\n",
    "\n",
    "        images = images.to(device, memory_format=torch.channels_last, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(images)['out']\n",
    "            loss = criterion(out, masks)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running_loss += loss.item()\n",
    "        loop.set_postfix(loss=running_loss/(loop.n+1), lr=f\"{current_lr:.6f}\")\n",
    "\n",
    "    train_elapsed = time.time() - start_time\n",
    "    avg_train_loss = running_loss/len(train_loader)\n",
    "    print(f\"\\nEpoch {epoch+1}/{start_epoch+epochs} - Avg Loss: {avg_train_loss:.4f} - Time: {train_elapsed:.2f} sec\")\n",
    "\n",
    "    # Validation during Training\n",
    "    # Pass 7 for num_classes_for_metric to evaluate mIoU over your 7 known macro classes (0-6).\n",
    "    val_loss, val_miou = evaluate_model(model, val_loader, device, num_classes_for_metric=7 , ignore_index=255)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation mIoU: {val_miou:.4f}\")\n",
    "\n",
    "    # Save Checkpoint (only if mIoU improves)\n",
    "    if val_miou > best_miou:\n",
    "        best_miou = val_miou\n",
    "        epochs_no_improve = 0 # Reset early stopping counter\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scaler': scaler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'best_miou': best_miou,\n",
    "            'current_iteration': current_iteration\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Model saved in {checkpoint_path} with improved mIoU: {best_miou:.4f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == 1:\n",
    "            print(f\"No improvement in mIoU for {epochs_no_improve} epoch. Best current: {best_miou:.4f}\")\n",
    "        else:\n",
    "            print(f\"No improvement in mIoU for {epochs_no_improve} epochs. Best current: {best_miou:.4f}\")\n",
    "        # Early Stopping\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
    "            break # Exiting the training loop\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d227ba9",
   "metadata": {},
   "source": [
    "### Evaluation (in our case with inference part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting Inference and Uncertainty Quantification\")\n",
    "\n",
    "# Load model for inference (ensure it's the same architecture as trained)\n",
    "model_inf = deeplabv3_resnet50(weights_backbone=ResNet50_Weights.IMAGENET1K_V1, num_classes=model_output_classes).to(device)\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint_inf = torch.load(checkpoint_path, map_location=device)\n",
    "    model_inf.load_state_dict(checkpoint_inf['model'])\n",
    "    print(\"Model loaded for inference.\")\n",
    "else:\n",
    "    print(f\"Checkpoint model not found in {checkpoint_path}. Impossible to proceed with inference.\")\n",
    "    exit() # Exit if model is not found, as subsequent steps depend on it\n",
    "\n",
    "model_inf.eval() # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053154ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conformal Prediction Calibration Step (using validation set as calibration data)\n",
    "# This step is crucial for Conformal Prediction to establish the 'q_hat' threshold.\n",
    "print(\"\\nConformal Prediction: Calibration phase for Inference\")\n",
    "calibration_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(val_loader, desc=\"Calibration\"): # Use val_loader as calibration set\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True) # Ground truth IDs (B, H, W)\n",
    "\n",
    "        outputs = model_inf(images)['out'] # Logits (B, C, H, W)\n",
    "        \n",
    "        # Resize outputs to original mask size if necessary (it should be resize already)\n",
    "        outputs = torch.nn.functional.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        probabilities = torch.sigmoid(outputs) # (B, C, H, W)\n",
    "\n",
    "        # Create a clamped version of masks for indexing.\n",
    "        # Any value 255 will be replaced by 0 (or any valid class ID, e.g., 0)\n",
    "        # This prevents \"index out of bounds\" in torch.gather.\n",
    "        clamped_masks = torch.where(masks == 255, torch.tensor(0, device=device, dtype=masks.dtype), masks)\n",
    "        \n",
    "        # For each pixel, get the probability of the true class\n",
    "        # masks has shape (B, H, W). unsqueeze(1) makes it (B, 1, H, W) for gather.\n",
    "        true_class_probs = torch.gather(probabilities, 1, clamped_masks.unsqueeze(1)).squeeze(1) # (B, H, W)\n",
    "\n",
    "        # Conformal Score: s_i = 1 - P(Y_true | X_i)\n",
    "        # Filter out ignore_index (255) from scores as they are not part of the ground truth for coverage\n",
    "        valid_pixels_mask = (masks != 255) # Use original masks for filtering\n",
    "        pixel_scores = (1 - true_class_probs)[valid_pixels_mask] # Only consider valid pixels\n",
    "\n",
    "        calibration_scores.extend(pixel_scores.cpu().numpy())\n",
    "\n",
    "calibration_scores = np.sort(calibration_scores)\n",
    "n_calib = len(calibration_scores)\n",
    "# Calculate the quantile q_hat: ceil((n+1)(1-alpha))/n empirical quantile\n",
    "# The +1 in numerator makes it non-asymptotic and valid (as per Angelopoulos et al. paper)\n",
    "# The index should be 0-based\n",
    "idx = int(np.ceil((n_calib + 1) * (1 - alpha))) - 1 # -1 for 0-based indexing\n",
    "q_hat = calibration_scores[idx] if idx < n_calib else (calibration_scores[-1] if n_calib > 0 else 0) # Handle edge case if idx out of bounds or empty\n",
    "print(f\"Calibration completed. Number of valid pixels for calibration: {n_calib}, q_hat: {q_hat:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Inference on an Example Image\n",
    "print(\"\\nInference Execution on Sample Image\")\n",
    "\n",
    "# Load and preprocess example image\n",
    "image = Image.open(example_image_path).convert('RGB')\n",
    "ground_truth_mask_ids = Image.open(example_ground_truth_path)\n",
    "\n",
    "# Resize images for consistency before processing\n",
    "image_resized = image.resize((resize[1], resize[0]), Image.BILINEAR)\n",
    "ground_truth_mask_ids_resized = ground_truth_mask_ids.resize((resize[1], resize[0]), Image.NEAREST)\n",
    "\n",
    "# Apply macro mapping directly to the loaded labelTrainIds\n",
    "# This requires creating a numpy mapping array as done in CityscapesFineDataset\n",
    "cityscapes_macro_mapping_array = np.full(256, UNKNOWN_OBSTACLE_ID, dtype=np.uint8) # Default to UNKNOWN_OBSTACLE_ID (7)\n",
    "for orig_id, macro_id in CITYSCAPES_19_TO_7_MACRO.items():\n",
    "    if 0 <= orig_id < 255:\n",
    "        cityscapes_macro_mapping_array[orig_id] = macro_id\n",
    "cityscapes_macro_mapping_array[255] = 255 # Ensure ignore_index remains 255\n",
    "mapped_ground_truth_mask_id = cityscapes_macro_mapping_array[np.array(ground_truth_mask_ids_resized)]\n",
    "    \n",
    "# Apply transformations for input image\n",
    "input_tensor = transform(image_resized).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_logits = model_inf(input_tensor)['out']\n",
    "    # Ensure output matches visualization size\n",
    "    output_logits = torch.nn.functional.interpolate(output_logits, size=resize, mode='bilinear', align_corners=False)\n",
    "    \n",
    "    probabilities = torch.sigmoid(output_logits).squeeze(0) # Shape: (C, H, W)\n",
    "\n",
    "    # 1. Predicted Segmentation (Known Classes ONLY for this visualization)\n",
    "    # Get argmax over known classes (0-6)\n",
    "    # This will give the most probable known class, but if UNKNOWN_OBSTACLE_ID (7)\n",
    "    # had the highest probability overall, it won't be reflected here directly.\n",
    "    predicted_known_only_argmax = torch.argmax(probabilities[:UNKNOWN_OBSTACLE_ID, :, :], dim=0).cpu().numpy()\n",
    "    \n",
    "    # Identify pixels where the model's overall highest probability was for the UNKNOWN_OBSTACLE_ID (7)\n",
    "    overall_prediction = torch.argmax(probabilities, dim=0) # Argmax over all channels (0-7)\n",
    "    is_overall_unknown = (overall_prediction == UNKNOWN_OBSTACLE_ID).cpu().numpy()\n",
    "\n",
    "    # For the \"Predicted Segmentation (Known Classes)\" visualization, set pixels predicted as unknown to ignore_id\n",
    "    # This makes them appear black in the plot, indicating they are not classified as known.\n",
    "    prediction_for_known_viz = predicted_known_only_argmax.copy()\n",
    "    prediction_for_known_viz[is_overall_unknown] = 255 # Set to ignore_id (black) for this specific visualization\n",
    "\n",
    "    # Unknown Objectness Score (UOS) Approximation (Noguchi et al.)\n",
    "    # Define \"object\" class IDs from our 7 macro classes (0-6).\n",
    "    # Based on CITYSCAPES_19_TO_7_MACRO:\n",
    "    # Macro Class 2: Human (person, rider) -> ID 2\n",
    "    # Macro Class 3: Vehicle Group -> ID 3\n",
    "    # Macro Class 5: Objects (pole, traffic light, traffic sign) -> ID 5\n",
    "    object_class_ids = [2, 3, 5]\n",
    "    \n",
    "    # Filter out any IDs that might be None or out of bounds.\n",
    "    # Crucially, ensure these are less than UNKNOWN_OBSTACLE_ID (7) as UOS is calculated over *known* objects.\n",
    "    object_class_ids = [idx for idx in object_class_ids if idx is not None and 0 <= idx < UNKNOWN_OBSTACLE_ID]\n",
    "    \n",
    "    # Approximate p_O (Objectness Score): Sum of probabilities for object classes\n",
    "    if object_class_ids:\n",
    "        # Sum over the probabilities of the identified object classes\n",
    "        # Use probabilities from channels corresponding to known objects\n",
    "        objectness_score = torch.sum(probabilities[object_class_ids, :, :], dim=0).cpu().numpy()\n",
    "    else:\n",
    "        objectness_score = np.zeros(probabilities.shape[1:], dtype=np.float32)\n",
    "        print(\"Warning: No object class IDs defined for UOS calculation. Objectness score will be zero.\")\n",
    "\n",
    "    # Calculate the product term: Product of (1 - p_ik) for all K known classes\n",
    "    # probabilities[:-1, :, :] correctly slices to include channels 0 through 6 (the 7 macro classes),\n",
    "    # explicitly excluding the last channel (index 7) which is for the UNKNOWN_OBSTACLE_ID.\n",
    "    product_term = torch.prod(1 - probabilities[:-1, :, :], dim=0).cpu().numpy()\n",
    "\n",
    "    # Calculate Unknown Objectness Score (UOS)\n",
    "    unknown_objectness_score = objectness_score * product_term\n",
    "    \n",
    "    # 2. Combined Prediction (Known + Unknown Obstacle)\n",
    "    # Start with the argmax over ALL classes (0-7)\n",
    "    combined_prediction = overall_prediction.cpu().numpy().copy()\n",
    "    # Then, if UOS is high, overwrite with UNKNOWN_OBSTACLE_ID (7)\n",
    "    is_anomaly_pixel = (unknown_objectness_score > anomaly_threshold_uos)\n",
    "    combined_prediction[is_anomaly_pixel] = UNKNOWN_OBSTACLE_ID # This will be ID 7\n",
    "\n",
    "    # Conformal Prediction Sets (Varisco Heatmap - Mossina et al.)\n",
    "    # Form pixel-wise prediction sets: C(x) = {y' | P(y'|x) >= 1 - q_hat}\n",
    "    p_threshold_conformal = 1 - q_hat\n",
    "\n",
    "    # varisco_heatmap will show the number of classes in the prediction set for each pixel\n",
    "    # `probabilities[:-1, :, :]` ensures only known classes (0-6) are considered for the prediction set size.\n",
    "    varisco_heatmap = torch.sum((probabilities[:-1, :, :] >= p_threshold_conformal), dim=0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32533da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Visualization\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(image_resized)\n",
    "plt.axis('off')\n",
    "\n",
    "# Predicted Segmentation (Known Classes ONLY)\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.title(\"Predicted Segmentation (Known Classes Only)\")\n",
    "# Use the specially prepared prediction_for_known_viz for this plot\n",
    "seg_image_known = decode_segmap(prediction_for_known_viz, colors_list=ALL_COLORS_FOR_VISUALIZATION[:UNKNOWN_OBSTACLE_ID]) # <--- Uses colors for 0-6\n",
    "plt.imshow(seg_image_known)\n",
    "plt.axis('off')\n",
    "\n",
    "# Ground Truth Segmentation\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.title(\"Ground Truth Segmentation\")\n",
    "# Decode GT using colors for known classes only (0-6)\n",
    "seg_image_gt = decode_segmap(mapped_ground_truth_mask_id, colors_list=ALL_COLORS_FOR_VISUALIZATION[:UNKNOWN_OBSTACLE_ID]) # <--- Uses colors for 0-6\n",
    "plt.imshow(seg_image_gt)\n",
    "plt.axis('off')\n",
    "\n",
    "# Anomaly Score (UOS) Map (Noguchi et al.)\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title(\"Anomaly Map (Unknown Objectness Score)\")\n",
    "plt.imshow(unknown_objectness_score, cmap='magma')\n",
    "plt.colorbar(label='UOS')\n",
    "plt.axis('off')\n",
    "\n",
    "# Combined Predicted Segmentation (Known + Unknown Obstacle)\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.title(f\"Combined Prediction (Known + Unknown Obstacle ID {UNKNOWN_OBSTACLE_ID})\")\n",
    "# Use ALL_COLORS_FOR_VISUALIZATION which includes the color for ID 7 (white)\n",
    "seg_image_combined = decode_segmap(combined_prediction, colors_list=ALL_COLORS_FOR_VISUALIZATION) # <--- Uses all 8 colors\n",
    "plt.imshow(seg_image_combined)\n",
    "plt.axis('off')\n",
    "\n",
    "# Conformal Prediction Set Size (Varisco Heatmap - Mossina et al.)\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.title(f\"Conformal Prediction Set Size (Î±={alpha})\")\n",
    "plt.imshow(varisco_heatmap, cmap='viridis')\n",
    "plt.colorbar(label='Number of classes in the set')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
