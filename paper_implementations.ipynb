{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d32c3b8",
   "metadata": {},
   "source": [
    "## Code based on the papers in the references of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62018e06",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms # Make sure this is imported\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from dataset import CityscapesFineDataset, COLORS, PALETTE2ID, DEFAULT_BG_CLASS, OLD_COLORS\n",
    "from torchmetrics.classification import MulticlassJaccardIndex\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc7a761",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebf04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\") if torch.cuda.is_available() else print(\"Using CPU\")\n",
    "\n",
    "# Training Parameters\n",
    "batch_size = 16 # Adjusted batch size for better performance\n",
    "model_output_classes = 7 #20 # Number of classes in Cityscapes dataset\n",
    "epochs = 1\n",
    "resize = (256, 512)\n",
    "checkpoint_path = 'deeplabv3_cityscapes_fine.pth'\n",
    "best_miou = 0.0 # To keep track of the best model based on validation mIoU\n",
    "\n",
    "# Optimizer Parameters (from \"Road Obstacle Detection based on Unknown Objectness Scores\" paper inspiration)\n",
    "initial_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "poly_power = 0.9 # Power for the \"poly\" learning rate policy\n",
    "\n",
    "# Conformal Prediction Parameters (from \"A Gentle Introduction to Conformal Prediction...\" paper)\n",
    "alpha = 0.1 # Alpha for Conformal Prediction (1 - alpha confidence level, e.g., 0.1 for 90% coverage)\n",
    "\n",
    "# Unknown Obstacle Detection Parameters\n",
    "UNKNOWN_OBSTACLE_ID = DEFAULT_BG_CLASS # Using num_classes as the ID for unknown obstacles\n",
    "ALL_COLORS_FOR_VISUALIZATION_GT = OLD_COLORS.tolist()\n",
    "ALL_COLORS_FOR_VISUALIZATION = COLORS.tolist() # This will pick up the COLORS from dataset.py\n",
    "anomaly_threshold_uos = 0.01 # Threshold for anomaly detection in unknown obstacle detection\n",
    "# Example image paths for inference\n",
    "example_image_path = 'datasets/realcityscapes/leftImg8bit/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png'\n",
    "example_ground_truth_path = 'datasets/realcityscapes/gtFine/val/frankfurt/frankfurt_000000_000294_gtFine_color.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8cd992",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bbab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_lr_scheduler(optimizer, initial_lr, current_iter, total_iter, power=0.9):\n",
    "    \"\"\"\n",
    "    Implements the \"poly\" learning rate policy: \n",
    "    lr = initial_lr * (1 - iter/total_iters)^power, clipped to avoid negatives.\n",
    "    \"\"\"\n",
    "    # Evita che current_iter superi total_iter\n",
    "    ratio = min(current_iter / total_iter, 1.0)\n",
    "    \n",
    "    # Calcolo LR (sempre positivo e reale)\n",
    "    lr = initial_lr * (1 - ratio) ** power\n",
    "    lr = max(float(lr), 1e-8)  # opzionale: evita LR troppo vicino a 0\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34faba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_segmap(pred_mask, colors_list=ALL_COLORS_FOR_VISUALIZATION, ignore_id=255):\n",
    "    \"\"\"\n",
    "    Decodes a segmentation mask (ID-based) into an RGB image using predefined colors.\n",
    "    Args:\n",
    "        pred_mask (np.array): Predicted segmentation mask (H, W) with class IDs.\n",
    "        colors_list (list): List of RGB color tuples/lists for each class ID.\n",
    "        ignore_id (int): The ID to be ignored (e.g., 255 for Cityscapes void pixels).\n",
    "    Returns:\n",
    "        np.array: RGB image (H, W, 3).\n",
    "    \"\"\"\n",
    "    h, w = pred_mask.shape\n",
    "    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for label_id in np.unique(pred_mask):\n",
    "        if label_id == ignore_id: # Handle ignore_index\n",
    "            color_mask[pred_mask == label_id] = [0, 0, 0] # Black for ignored pixels\n",
    "        elif 0 <= label_id < len(colors_list):\n",
    "            color_mask[pred_mask == label_id] = colors_list[label_id]\n",
    "        else:\n",
    "            # Fallback for unexpected labels (shouldn't happen if UNKNOWN_OBSTACLE_ID is handled correctly)\n",
    "            print(f\"Warning: Unexpected label ID {label_id} found in prediction mask. Mapping to blue.\")\n",
    "            color_mask[pred_mask == label_id] = [0, 0, 255] # Blue for unmapped unexpected\n",
    "    return color_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb22726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_id(rgb_mask, palette2id=PALETTE2ID, default=255):\n",
    "    \"\"\"\n",
    "    Converts an RGB mask image to a single-channel ID mask using a palette mapping.\n",
    "    Args:\n",
    "        rgb_mask (np.array): RGB mask image (H, W, 3).\n",
    "        palette2id (dict): Dictionary mapping RGB tuples to class IDs.\n",
    "        default (int): Default ID for colors not found in the palette.\n",
    "    Returns:\n",
    "        np.array: Single-channel ID mask (H, W).\n",
    "    \"\"\"\n",
    "    # Ensure rgb_mask is 3 channels\n",
    "    if rgb_mask.ndim == 2: # Handle grayscale images by adding a channel\n",
    "        rgb_mask = np.stack([rgb_mask]*3, axis=-1)\n",
    "    \n",
    "    # Create a 2D array for IDs, initialized with the default (ignore) value\n",
    "    id_mask = np.full(rgb_mask.shape[:2], default, dtype=np.uint8)\n",
    "    \n",
    "    # Iterate through the palette and assign IDs\n",
    "    for rgb_tuple, id_val in palette2id.items():\n",
    "        # Find pixels that match the current RGB tuple\n",
    "        matches = np.all(rgb_mask == np.array(rgb_tuple, dtype=np.uint8), axis=-1)\n",
    "        id_mask[matches] = id_val\n",
    "    \n",
    "    return id_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9af173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, num_classes, ignore_index=255):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a given dataloader and computes the average loss and mIoU.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Initialize JaccardIndex (IoU) metric.\n",
    "    metric = MulticlassJaccardIndex(num_classes=num_classes, ignore_index=ignore_index, average='macro', validate_args=False).to(device)\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations for inference\n",
    "        for images, masks in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(images)['out']\n",
    "            # Use the global criterion (defined in Train section)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1) # Get predicted class for each pixel\n",
    "            metric.update(preds, masks) # Update the metric with predictions and ground truth\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    mean_iou = metric.compute() # Compute final mIoU across all batches\n",
    "\n",
    "    model.train() # Set model back to training mode\n",
    "    return avg_loss, mean_iou.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1aa25",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations for input images\n",
    "transform = transforms.Compose([\n",
    "    # ColorJitter is for images only, keep it here.\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(), # Convert PIL Image to Tensor\n",
    "    # Normalization for ImageNet pre-trained models (DeepLabV3 uses ResNet50 backbone)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Using CityscapesFineDataset from dataset.py\n",
    "train_ds = CityscapesFineDataset(root='datasets/realcityscapes', split='train', transform=transform, resize=resize)\n",
    "val_ds = CityscapesFineDataset(root='datasets/realcityscapes', split='val', transform=transform, resize=resize)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8, # This can be adjusted based on your system's capabilities\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True, # Set to True for better performance on systems with multiple workers\n",
    "    prefetch_factor=4, # Prefetch factor for better performance\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, # No need to shuffle validation data\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4,\n",
    ")\n",
    "print(\"Datasets loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40d734",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DeepLabV3 with ImageNet pre-trained weights for the ResNet50 backbone.\n",
    "# This helps in transfer learning and often leads to faster convergence and better performance.\n",
    "model = deeplabv3_resnet50(weights_backbone=ResNet50_Weights.IMAGENET1K_V1, num_classes=model_output_classes).to(device)\n",
    "model = model.to(memory_format=torch.channels_last) # Optimize memory layout for NVIDIA GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b6ae5b",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer: SGD with momentum and weight decay as specified in the paper\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255) # ignore_index=255 for unlabeled pixels\n",
    "scaler = torch.cuda.amp.GradScaler() # For mixed precision training\n",
    "\n",
    "# Checkpoint Handling\n",
    "start_epoch = 0\n",
    "current_iteration = 0 # Initialize current_iteration for learning rate scheduler\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    ck = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(ck['model'])\n",
    "    optimizer.load_state_dict(ck['optimizer'])\n",
    "    scaler.load_state_dict(ck['scaler'])\n",
    "    start_epoch = ck['epoch'] + 1\n",
    "    if 'best_miou' in ck:\n",
    "        best_miou = ck['best_miou']\n",
    "    if 'current_iteration' in ck:\n",
    "        current_iteration = ck['current_iteration']\n",
    "    print(f\"Restarting from epoch {start_epoch}, best mIoU until now: {best_miou:.4f}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting training from zero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "print(\"Starting training...\")\n",
    "total_iterations = epochs * len(train_loader) # Define total_iterations here for scheduler\n",
    "\n",
    "patience = 5 # Patience for early stopping\n",
    "epochs_no_improve = 0 # Counter for early stopping\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    start_time = time.time() # Start timer for epoch\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{start_epoch+epochs}\")\n",
    "    for images, masks in loop:\n",
    "        current_iteration += 1\n",
    "        current_lr = poly_lr_scheduler(optimizer, initial_lr, current_iteration, total_iterations, power=poly_power)\n",
    "\n",
    "        images = images.to(device, memory_format=torch.channels_last, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(images)['out']\n",
    "            loss = criterion(out, masks)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running_loss += loss.item()\n",
    "        loop.set_postfix(loss=running_loss/(loop.n+1), lr=f\"{current_lr:.6f}\") # Display current loss and LR\n",
    "\n",
    "    train_elapsed = time.time() - start_time\n",
    "    avg_train_loss = running_loss/len(train_loader)\n",
    "    print(f\"\\nEpoch {epoch+1}/{start_epoch+epochs} - Avg Loss: {avg_train_loss:.4f} - Time: {train_elapsed:.2f} sec\")\n",
    "\n",
    "    # Validation during Training\n",
    "    # `evaluate_model` is defined in the Utils section\n",
    "    val_loss, val_miou = evaluate_model(model, val_loader, device, model_output_classes , ignore_index=255)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation mIoU: {val_miou:.4f}\")\n",
    "\n",
    "    # Save Checkpoint (only if mIoU improves)\n",
    "    if val_miou > best_miou:\n",
    "        best_miou = val_miou\n",
    "        epochs_no_improve = 0 # Reset early stopping counter\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scaler': scaler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'best_miou': best_miou,\n",
    "            'current_iteration': current_iteration\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Model saved in {checkpoint_path} with improved mIoU: {best_miou:.4f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == 1:\n",
    "            print(f\"No improvement in mIoU for {epochs_no_improve} epoch. Best current: {best_miou:.4f}\")\n",
    "        else:\n",
    "            print(f\"No improvement in mIoU for {epochs_no_improve} epochs. Best current: {best_miou:.4f}\")\n",
    "        # Early Stopping\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
    "            break # Exiting the training loop\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d227ba9",
   "metadata": {},
   "source": [
    "### Evaluation (in our case with inference part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting Inference and Uncertainty Quantification\")\n",
    "\n",
    "# Load model for inference (ensure it's the same architecture as trained)\n",
    "model_inf = deeplabv3_resnet50(weights_backbone=ResNet50_Weights.IMAGENET1K_V1, num_classes=model_output_classes).to(device)\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint_inf = torch.load(checkpoint_path, map_location=device)\n",
    "    model_inf.load_state_dict(checkpoint_inf['model'])\n",
    "    print(\"Model loaded for inference.\")\n",
    "else:\n",
    "    print(f\"Checkpoint model not found in {checkpoint_path}. Impossible to proceed with inference.\")\n",
    "    exit() # Exit if model is not found, as subsequent steps depend on it\n",
    "\n",
    "model_inf.eval() # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053154ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conformal Prediction Calibration Step (using validation set as calibration data)\n",
    "# This step is crucial for Conformal Prediction to establish the 'q_hat' threshold.\n",
    "print(\"\\nConformal Prediction: Calibration phase for Inference\")\n",
    "calibration_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(val_loader, desc=\"Calibration\"): # Use val_loader as calibration set\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True) # Ground truth IDs (B, H, W)\n",
    "\n",
    "        outputs = model_inf(images)['out'] # Logits (B, C, H, W)\n",
    "        \n",
    "        # Resize outputs to original mask size if necessary (it should be resize already)\n",
    "        outputs = torch.nn.functional.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        probabilities = torch.softmax(outputs, dim=1) # (B, C, H, W)\n",
    "\n",
    "        # Create a clamped version of masks for indexing.\n",
    "        # Any value 255 will be replaced by 0 (or any valid class ID, e.g., 0)\n",
    "        # This prevents \"index out of bounds\" in torch.gather.\n",
    "        clamped_masks = torch.where(masks == 255, torch.tensor(0, device=device, dtype=masks.dtype), masks)\n",
    "        \n",
    "        # For each pixel, get the probability of the true class\n",
    "        # masks has shape (B, H, W). unsqueeze(1) makes it (B, 1, H, W) for gather.\n",
    "        true_class_probs = torch.gather(probabilities, 1, clamped_masks.unsqueeze(1)).squeeze(1) # (B, H, W)\n",
    "\n",
    "        # Conformal Score: s_i = 1 - P(Y_true | X_i)\n",
    "        # Filter out ignore_index (255) from scores as they are not part of the ground truth for coverage\n",
    "        valid_pixels_mask = (masks != 255) # Use original masks for filtering\n",
    "        pixel_scores = (1 - true_class_probs)[valid_pixels_mask] # Only consider valid pixels\n",
    "\n",
    "        calibration_scores.extend(pixel_scores.cpu().numpy())\n",
    "\n",
    "calibration_scores = np.sort(calibration_scores)\n",
    "n_calib = len(calibration_scores)\n",
    "# Calculate the quantile q_hat: ceil((n+1)(1-alpha))/n empirical quantile\n",
    "# The +1 in numerator makes it non-asymptotic and valid (as per Angelopoulos et al. paper)\n",
    "# The index should be 0-based\n",
    "idx = int(np.ceil((n_calib + 1) * (1 - alpha))) - 1 # -1 for 0-based indexing\n",
    "q_hat = calibration_scores[idx] if idx < n_calib else (calibration_scores[-1] if n_calib > 0 else 0) # Handle edge case if idx out of bounds or empty\n",
    "print(f\"Calibration completed. Number of valid pixels for calibration: {n_calib}, q_hat: {q_hat:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Inference on an Example Image\n",
    "print(\"\\nInference Execution on Sample Image\")\n",
    "\n",
    "# Load and preprocess example image\n",
    "image = Image.open(example_image_path).convert('RGB')\n",
    "ground_truth_mask_rgb = Image.open(example_ground_truth_path).convert('RGB')\n",
    "\n",
    "# Resize images for consistency before processing\n",
    "image_resized = image.resize((resize[1], resize[0]), Image.BILINEAR)\n",
    "ground_truth_mask_rgb_resized = ground_truth_mask_rgb.resize((resize[1], resize[0]), Image.NEAREST)\n",
    "\n",
    "# Convert ground truth RGB mask to ID mask for comparison/display\n",
    "mapped_ground_truth_mask_id = rgb_to_id(np.array(ground_truth_mask_rgb_resized), palette2id=PALETTE2ID, default=255)\n",
    "    \n",
    "# Apply transformations for input image\n",
    "input_tensor = transform(image_resized).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_logits = model_inf(input_tensor)['out']\n",
    "    # Ensure output matches visualization size\n",
    "    output_logits = torch.nn.functional.interpolate(output_logits, size=resize, mode='bilinear', align_corners=False)\n",
    "    \n",
    "    probabilities = torch.softmax(output_logits, dim=1).squeeze(0) # Shape: (C, H, W)\n",
    "\n",
    "    # 1. Predicted Segmentation (Known Classes)\n",
    "    prediction_known_classes = torch.argmax(probabilities, dim=0).cpu().numpy() # Shape: (H, W)\n",
    "\n",
    "    # Unknown Objectness Score (UOS) Approximation (Noguchi et al.)\n",
    "    # Define \"object\" class IDs from your 19 Cityscapes classes.\n",
    "    # This is based on common Cityscapes object categories (from PALETTE2ID).\n",
    "    # Controlla num_classes dal tensore delle probabilità\n",
    "    num_classes = probabilities.shape[0]\n",
    "\n",
    "    if num_classes == DEFAULT_BG_CLASS+1:  # If using the 7-class model\n",
    "        # Mapping 7-classi\n",
    "        object_class_ids = [2, 3, 5]  # human, vehicle, object\n",
    "    else:\n",
    "        # Mapping 19-classi originale (usa PALETTE2ID)\n",
    "        object_class_ids = [\n",
    "            PALETTE2ID.get((0, 0, 142)),   # car\n",
    "            PALETTE2ID.get((0, 0, 70)),    # truck\n",
    "            PALETTE2ID.get((0, 60, 100)),  # bus\n",
    "            PALETTE2ID.get((0, 80, 100)),  # train\n",
    "            PALETTE2ID.get((0, 0, 230)),   # motorcycle\n",
    "            PALETTE2ID.get((119, 11, 32)), # bicycle\n",
    "            PALETTE2ID.get((220, 20, 60)), # person\n",
    "            PALETTE2ID.get((255, 0, 0)),   # rider\n",
    "            PALETTE2ID.get((250, 170, 30)),# traffic light\n",
    "            PALETTE2ID.get((220, 220, 0))  # traffic sign\n",
    "        ]\n",
    "\n",
    "    # Filtra eventuali None e indici fuori range\n",
    "    object_class_ids = [\n",
    "        c for c in object_class_ids if c is not None and c < num_classes\n",
    "    ]\n",
    "\n",
    "    # Calcola objectness_score in modo sicuro\n",
    "    if object_class_ids:\n",
    "        objectness_score = torch.sum(probabilities[object_class_ids, :, :].cpu(), dim=0).numpy()\n",
    "    else:\n",
    "        objectness_score = np.zeros(probabilities.shape[1:], dtype=np.float32)\n",
    "\n",
    "\n",
    "    # Calculate the product term: Product of (1 - p_ik)\n",
    "    # `probabilities[:-1, :, :]` slices the tensor to include only channels 0 through 18, excluding the new unknown class at index 19. for all K known classes\n",
    "    product_term = torch.prod(1 - probabilities[:-1, :, :], dim=0).cpu().numpy() # Shape: (H, W)\n",
    "\n",
    "    # Calculate Unknown Objectness Score (UOS)\n",
    "    unknown_objectness_score = objectness_score * product_term\n",
    "    \n",
    "    combined_prediction = prediction_known_classes.copy()\n",
    "    is_anomaly_pixel = (unknown_objectness_score > anomaly_threshold_uos)\n",
    "    combined_prediction[is_anomaly_pixel] = UNKNOWN_OBSTACLE_ID\n",
    "\n",
    "\n",
    "    # Conformal Prediction Sets (Varisco Heatmap - Mossina et al.)\n",
    "    # Form pixel-wise prediction sets: C(x) = {y' | P(y'|x) >= 1 - q_hat}\n",
    "    p_threshold_conformal = 1 - q_hat\n",
    "\n",
    "    # varisco_heatmap will show the number of classes in the prediction set for each pixel\n",
    "    # probabilities shape: (C, H, W)\n",
    "    varisco_heatmap = torch.sum((probabilities[:-1, :, :] >= p_threshold_conformal), dim=0).cpu().numpy()\n",
    "    # Again, `probabilities[:-1, :, :]` ensures only known classes are considered for the prediction set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32533da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Visualization\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 12)) # Increased figure size for better display\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(image_resized)\n",
    "plt.axis('off')\n",
    "\n",
    "# Predicted Segmentation (Known Classes Only)\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.title(\"Predicted Segmentation (Known Classes)\")\n",
    "seg_image_known = decode_segmap(prediction_known_classes, colors_list=ALL_COLORS_FOR_VISUALIZATION)\n",
    "plt.imshow(seg_image_known)\n",
    "plt.axis('off')\n",
    "\n",
    "# Ground Truth Segmentation\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.title(\"Ground Truth Segmentation\")\n",
    "# Decode GT using only known colors, as GT should not have 'unknown'\n",
    "seg_image_gt = decode_segmap(mapped_ground_truth_mask_id, colors_list=ALL_COLORS_FOR_VISUALIZATION_GT)#[:UNKNOWN_OBSTACLE_ID])\n",
    "plt.imshow(seg_image_gt)\n",
    "plt.axis('off')\n",
    "\n",
    "# Anomaly Score (UOS) Map (Noguchi et al.)\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title(\"Anomaly Map (Unknown Objectness Score)\")\n",
    "plt.imshow(unknown_objectness_score, cmap='magma') # 'magma' or 'hot' colormap is good for showing intensity\n",
    "plt.colorbar(label='UOS')\n",
    "plt.axis('off')\n",
    "\n",
    "# Combined Predicted Segmentation (Known + Unknown Obstacle)\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.title(f\"Combined Prediction (Known + Unknown Obstacle ID {UNKNOWN_OBSTACLE_ID})\")\n",
    "# Use ALL_COLORS_FOR_VISUALIZATION which includes the new magenta for ID 19\n",
    "seg_image_combined = decode_segmap(combined_prediction, colors_list=ALL_COLORS_FOR_VISUALIZATION)\n",
    "plt.imshow(seg_image_combined)\n",
    "plt.axis('off')\n",
    "\n",
    "# Conformal Prediction Set Size (Varisco Heatmap - Mossina et al.)\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.title(f\"Conformal Prediction Set Size (α={alpha})\")\n",
    "# Use a colormap that clearly shows varying counts, e.g., 'viridis' or 'plasma'\n",
    "plt.imshow(varisco_heatmap, cmap='viridis')\n",
    "plt.colorbar(label='Number of classes in the set')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnalysis Completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
